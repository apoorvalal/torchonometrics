{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from gmm import GELEstimator\n",
    "np.random.seed(94305)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Empirical Likelihood Estimation\n",
    "\n",
    "We want to estimate a parameter implicitly defined as a solution to a moment condition $g(Z; \\theta) = 0$. \n",
    "\n",
    "GMM proceeds by minimizing the quadratic form $Q(\\theta) = g(Z; \\theta)'Wg(Z; \\theta)$, where $W$ is a positive definite weighting matrix. The choice of the weighting matrix is crucial for the efficiency of the estimator.\n",
    "\n",
    "GEL sidesteps the choice of the weighting matrix by minimizing the empirical likelihood function. The empirical likelihood function is defined as the maximum of the likelihood function subject to the moment condition.\n",
    "\n",
    "The (log) empirical likelihood function is defined as:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\max_{p \\in \\mathcal{P}} \\sum_{i=1}^n \\log p_i\n",
    "$$\n",
    "\n",
    "subject to the moment condition $$p_i g(Z; \\theta) = 0$$\n",
    "and adding up $\\sum_{i=1}^n p_i = 1$.\n",
    "\n",
    "where $\\mathcal{P}$ is the set of all probability distributions on $\\{1, \\ldots, n\\}$. At a first glance, this problem seems harder, since we need to solve for a weight vector $p$, which is typically of much higher dimension than the parameter $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Imbens, Spady, and Johnson (1998)](https://scholar.harvard.edu/sites/scholar.harvard.edu/files/imbens/files/information_theoretic_approaches_to_inference_in_moment_condition_models.pdf) show that minimizing the KLIC instead of the log empirical likelihood produces better behavior under `mild' misspecification. The corresponding Exponential Tilting estimator is defined as solving\n",
    "\n",
    "$$\n",
    "\\max_{\\theta, p_i} - \\sum_{i=1}^n p_i \\log p_i\n",
    "$$\n",
    "\n",
    "subject to the moment condition $p_i g(Z; \\theta) = 0$ and $\\sum_{i=1}^n p_i = 1$. \n",
    "\n",
    "The estimating equations are difficult to solve directly, so we instead use the saddlepoint representation\n",
    "\n",
    "$$\n",
    "\\max_{\\theta} \\min_{t} \\sum_{i=1}^n \\exp(t' g(Z_i; \\theta))\n",
    "$$\n",
    "\n",
    "that concentrates out the weight vector.\n",
    "The inner function is strictly convex in $t$, and can be solved quickly. The outer function iterates over candidate values of $\\theta$. We implement this approach in `GELEstimator`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Problems\n",
    "\n",
    "### estimating a mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01999056, 0.03015686],\n",
       "       [2.04451165, 0.03196784]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1000\n",
    "X = np.random.normal(np.array([1, 2]), size=(n, 2))\n",
    "np.c_[X.mean(axis=0), np.sqrt(n / (n - 1) * X.var(axis=0) / n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01999054, 0.03015686],\n",
       "       [2.04451167, 0.03196784]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moment_cond_mean = lambda D, theta: D - theta\n",
    "\n",
    "gelmod = GELEstimator(m = moment_cond_mean)\n",
    "gelmod.fit(X, np.zeros(2))\n",
    "gelmod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytic and GEL estimates identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgp(n=100_000, beta=np.array([-0.5, 1.2]), rho=0.7, pi=np.array([0.5, -0.1])):\n",
    "    ε = np.random.normal(0, 1, n)\n",
    "    z = np.random.normal(0, 1, n * pi.shape[0]).reshape(n, pi.shape[0])\n",
    "    # Generate endogenous x, influenced by the instrument\n",
    "    x = z @ pi + ε * rho + np.random.normal(0, 1, n)\n",
    "    X = np.c_[np.ones(n), x]\n",
    "    # heteroskedasticity\n",
    "    y = (\n",
    "        X @ beta\n",
    "        + ε\n",
    "        + np.random.normal(0, 1 + 2 * (X[:, 1] > 0) + 1 * (np.abs(X[:, 1]) > 0.5), n)\n",
    "    )\n",
    "    return y, X, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.4980      0.010    -51.823      0.000      -0.517      -0.479\n",
      "x1             1.2048      0.009    130.295      0.000       1.187       1.223\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "y, X, z = dgp(rho = 0)\n",
    "print(sm.OLS(y, X).fit(cov_type = \"HC2\").summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Estimation via EL/ET__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.49799059,  0.00961084],\n",
       "       [ 1.20481178,  0.01167324]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def moment_condition_ols(D, θ):\n",
    "    y, X = D[:, 0], D[:, 1:]\n",
    "    r = y - X @ θ\n",
    "    return X * r[:, None]\n",
    "\n",
    "\n",
    "D = np.c_[y, X]\n",
    "gelmod = GELEstimator(m=moment_condition_ols, verbose=False)\n",
    "k = X.shape[1]\n",
    "gelmod.fit(D, np.zeros(k))\n",
    "gelmod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.5031      0.010    -52.128      0.000      -0.522      -0.484\n",
      "x1             1.6453      0.007    241.302      0.000       1.632       1.659\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "y, X, Z = dgp(rho=1)\n",
    "print(sm.OLS(y, X).fit(cov_type=\"HC2\").summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2129555645196368, 0.01892188485776335)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2SLS via control function\n",
    "D = X[:, 1]\n",
    "Dhat = sm.OLS(D, Z).fit().predict()\n",
    "tslsreg = sm.OLS(y, np.c_[sm.add_constant(D), D - Dhat]).fit(cov_type=\"HC1\")\n",
    "tslsreg.params[1], tslsreg.bse[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unbiased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52258708, 0.01041044],\n",
       "       [1.22171316, 0.01039752]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def moment_condition_iv(D, θ):\n",
    "    y, X, Z = D[:, 0], D[:, 1:3], D[:, 3:]\n",
    "    r = y - X @ θ\n",
    "    return Z * r[:, None]\n",
    "\n",
    "\n",
    "D = np.c_[y, X, Z]\n",
    "k = X.shape[1]\n",
    "gelmod = GELEstimator(m=moment_condition_iv)\n",
    "gelmod.fit(D, np.zeros(k))\n",
    "gelmod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Endogenous coefficient is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Problems \n",
    "\n",
    "### Lin Regression\n",
    "\n",
    "$$\n",
    "Y_i = \\alpha + \\tau Z_i + X_i' \\beta + Z_i \\tilde{X}_i' \\gamma + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $\\tilde{X}_i = X_i - \\bar{X}$ is the centered version of $X_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "X = np.random.normal(0, 1, n * 2).reshape(n, 2)\n",
    "Y0 = X[:, 0] + X[:, 0] ** 2 + np.random.uniform(-0.5, 0.5, n)\n",
    "Y1 = X[:, 1] + X[:, 1] ** 2 + np.random.uniform(-1, 1, n)\n",
    "Z = np.random.binomial(1, 0.6, n)\n",
    "Y = Y0 * (1 - Z) + Y1 * Z\n",
    "D = np.c_[Y, Z, X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    1.0189</td> <td>    0.098</td> <td>   10.413</td> <td> 0.000</td> <td>    0.827</td> <td>    1.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.1907</td> <td>    0.139</td> <td>    1.371</td> <td> 0.170</td> <td>   -0.082</td> <td>    0.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.9860</td> <td>    0.191</td> <td>    5.167</td> <td> 0.000</td> <td>    0.612</td> <td>    1.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.1129</td> <td>    0.101</td> <td>    1.114</td> <td> 0.265</td> <td>   -0.086</td> <td>    0.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.8281</td> <td>    0.218</td> <td>   -3.794</td> <td> 0.000</td> <td>   -1.256</td> <td>   -0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    1.4133</td> <td>    0.213</td> <td>    6.633</td> <td> 0.000</td> <td>    0.996</td> <td>    1.831</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\toprule\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       1.0189  &        0.098     &    10.413  &         0.000        &        0.827    &        1.211     \\\\\n",
       "\\textbf{x1}    &       0.1907  &        0.139     &     1.371  &         0.170        &       -0.082    &        0.463     \\\\\n",
       "\\textbf{x2}    &       0.9860  &        0.191     &     5.167  &         0.000        &        0.612    &        1.360     \\\\\n",
       "\\textbf{x3}    &       0.1129  &        0.101     &     1.114  &         0.265        &       -0.086    &        0.311     \\\\\n",
       "\\textbf{x4}    &      -0.8281  &        0.218     &    -3.794  &         0.000        &       -1.256    &       -0.400     \\\\\n",
       "\\textbf{x5}    &       1.4133  &        0.213     &     6.633  &         0.000        &        0.996    &        1.831     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olslin = sm.OLS(\n",
    "    Y, np.c_[sm.add_constant(Z), X, Z[:, None] * (X - X.mean(axis=0))]\n",
    ").fit(cov_type = \"HC2\")\n",
    "olslin.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point estimates are consistent, but we do not propagate forward the uncertainty from estimating the sample mean $\\bar{X}$. To do this, we could stack the moment conditions\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "X - \\mu \\\\\n",
    "[X, X - \\mu] (y - [X, X - \\mu] \\beta)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02681823 0.0657933  0.16969411 0.20839162 0.96615393 0.1186393\n",
      " 0.76986621 0.31422366]\n"
     ]
    }
   ],
   "source": [
    "def moment_condition_lin(D, θ):\n",
    "    Y, Z, X = D[:, 0], D[:, 1], D[:, 2:]\n",
    "    n, p = X.shape\n",
    "    mu, beta = θ[:p], θ[p:]\n",
    "    Xcent = X - mu\n",
    "    XX = np.c_[np.ones(n), Z, X, Z[:, None] * Xcent]\n",
    "    r = XX * (Y - XX @ beta)[:, None]\n",
    "    return np.c_[Xcent, r]\n",
    "\n",
    "\n",
    "print(theta_init := np.r_[X.mean(axis=0), np.random.rand(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gelmod = GELEstimator(m=moment_condition_lin, min_method=\"COBYLA\")\n",
    "gelmod.fit(D, theta_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02047514,  0.04419224,  0.02681823],\n",
       "       [ 0.067958  ,  0.04785572,  0.0657933 ],\n",
       "       [ 1.01598886,  0.07163751,  1.01894042],\n",
       "       [ 0.20515589,  0.06153608,  0.1907456 ],\n",
       "       [ 0.91805187,  0.0943588 ,  0.98603771],\n",
       "       [ 0.11959629,  0.14415267,  0.1128657 ],\n",
       "       [-0.74880967,  0.0628265 , -0.82811793],\n",
       "       [ 1.42066068,  0.13805289,  1.41325515]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.c_[gelmod.summary(), np.r_[X.mean(axis = 0), olslin.params]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter vector has $\\bar{X}$ as the first two elements, and the later are the regression coefficients. These look about right, although the standard errors are a bit too narrow?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
